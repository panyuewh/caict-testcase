{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pyarrow as pa \n",
    "import pyarrow.parquet as pq\n",
    "import numpy as np\n",
    "import math\n",
    "import hashlib\n",
    "\n",
    "import os\n",
    "path = './ds'\n",
    "if not os.path.exists(path):\n",
    "    os.mkdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 数据集α: 千万级 | 7 | 基础运算、联合统计\n",
    "# 通过Python脚本生成三份一千万行的随机浮点数(float32)样本集。\n",
    "# 每份样本集包含一列ID（对1到10000000进行32位的MD5加密处理），两列特征(样本1为X1、X2；样本2为X3、X4；样本3为X5、X6)，\n",
    "# 其中三份样本的ID列相同，X1、X2、X3、X4、X5、X6为随机生成的浮点数（范围从1.0-1000.0）。\n",
    "# X1、X3、X5用于基础运算，X2、X4、X6用于联合统计。\n",
    "\n",
    "def md5(i: int):\n",
    "    md = hashlib.md5()\n",
    "    md.update(str(i).encode('utf-8'))\n",
    "    return md.hexdigest()\n",
    "\n",
    "len_alpha = 10000000\n",
    "ID_alpha = [md5(i) for id in np.arange(1, len_alpha + 1)]\n",
    "for x in [0, 1, 2]:\n",
    "    df = pd.DataFrame({'ID': ID_alpha,\n",
    "                       f'X{x * 2 + 1}': np.random.random((len_alpha,)) * 999 + 1,\n",
    "                       f'X{x * 2 + 2}': np.random.random((len_alpha,)) * 999 + 1\n",
    "                      })\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(table, root_path=f'{path}/ds_alpha{x+1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# 数据集β: 亿级别 | 4 | PIR\n",
    "# 数据集由脚本随机生成，每条样本包含ID、注册日期、年龄、消费金额四个特征。\n",
    "# 其中ID为18位十进制的随机整数，\n",
    "# 注册日期为2000-01-01到2020-12-31范围的随机年月日数值，\n",
    "# 年龄为15到80之间的随机整数，\n",
    "# 消费金额为0.00-1000000.00范围内的随机浮点数(小数点后保留2位)。\n",
    "# 从被查询数据集中随机抽取10000个ID值，作为高效率查询（百级不可区分度）的待查询ID。\n",
    "# 从被查询数据集中随机抽取1个ID值，作为高隐匿性查询（百万级不可区分度）的待查询ID。\n",
    "\n",
    "len_beta = 100000000\n",
    "batch_size = 1000000\n",
    "query_ratio = 1000 / len_beta\n",
    "batch = math.ceil(len_beta / batch_size)\n",
    "for i in range(0, batch):\n",
    "    print(f\"{i}/{batch}\", end=\": \")\n",
    "    ID_beta_array = np.random.choice(bytearray(b'0123456789'), size=(batch_size, 18))\n",
    "    ID_beta = ID_beta_array.view(dtype='|S18').flat\n",
    "    date_start = np.datetime64('2000-01-01')\n",
    "    days = np.arange(0, np.datetime64('2020-12-31') - date_start)\n",
    "    Date_beta = date_start + np.random.choice(days, size=(batch_size,))\n",
    "    df = pd.DataFrame({'ID': ID_beta,\n",
    "                    'Date': Date_beta,\n",
    "                    'Age': np.random.randint(15, 80, size=(batch_size,)),\n",
    "                    'Amount': np.random.random(size=(batch_size,)) * 1000000\n",
    "                    })\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(table, root_path=f'{path}/ds_beta')\n",
    "    print(\"ds_beta\", end=\",\")\n",
    "    df_query = pd.DataFrame({'ID': np.random.choice(ID_beta, (math.ceil(batch_size * query_ratio),))})\n",
    "    table_query = pa.Table.from_pandas(df_query)\n",
    "    pq.write_to_dataset(table_query, root_path=f'{path}/ds_beta_query')\n",
    "    print(\"ds_beta_query\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "# 数据集γ: 亿级别 | 1 | PSI\n",
    "# 求交数据集由脚本随机生成，每个样本包含一个ID信息，ID为18位十进制的随机整数。不同场景各数据方的数据集规模不同。\n",
    "# a）两方平衡场景：数据方A、数据方B的数据总量均为一亿行，两方数据的相交率为50%，即五千万条相同ID。\n",
    "# b）两方非平衡场景：数据方A数据总量为一亿行、数据方B的数据总量为十万行，两方数据的相交率为50%，即五万条相同ID。\n",
    "\n",
    "len_gamma = 100000000\n",
    "batch_size = 1000000\n",
    "join_rate = 0.5\n",
    "query_ratio = 100000 / len_gamma\n",
    "rng = np.random.default_rng()\n",
    "batch = math.ceil(len_gamma / batch_size)\n",
    "for i in range(0, batch):\n",
    "    print(f\"{i}/{batch}\", end=\": \")\n",
    "    ID_gamma_array = np.random.choice(bytearray(b'0123456789'), size=(batch_size, 18))\n",
    "    ID_gamma = ID_gamma_array.view(dtype='|S18').flatten()\n",
    "    df = pd.DataFrame({'ID': ID_gamma})\n",
    "    table = pa.Table.from_pandas(df)\n",
    "    pq.write_to_dataset(table, root_path=f'{path}/ds_gamma')\n",
    "    print(\"ds_gamma\", end=\", \")\n",
    "    # query a)\n",
    "    ID_gamma_query_a1 = np.random.choice(ID_gamma, \n",
    "                                         size=math.ceil(batch_size * join_rate,)\n",
    "                                        )\n",
    "    ID_gamma_query_a2 = np.random.choice(bytearray(b'0123456789'), \n",
    "                                         size=(batch_size - math.ceil(batch_size * join_rate), 18)\n",
    "                                        )\n",
    "    ID_gamma_query_a = np.concatenate((ID_gamma_query_a1, ID_gamma_query_a2.view(dtype='|S18').flatten()))\n",
    "    rng.shuffle(ID_gamma_query_a)\n",
    "    df_query_a = pd.DataFrame({'ID': ID_gamma_query_a })\n",
    "    table_query_a = pa.Table.from_pandas(df_query_a)\n",
    "    pq.write_to_dataset(table_query_a, root_path=f'{path}/ds_gamma_query_a')\n",
    "    print(\"ds_gamma_query_a\", end=\", \")\n",
    "    # query b)\n",
    "    batch_size_query_b = math.ceil(batch_size * query_ratio)\n",
    "    ID_gamma_query_b1 = np.random.choice(ID_gamma, \n",
    "                                         size=math.ceil(batch_size_query_b * join_rate,)\n",
    "                                        )\n",
    "    ID_gamma_query_b2 = np.random.choice(bytearray(b'0123456789'), \n",
    "                                         size=(batch_size_query_b - math.ceil(batch_size_query_b * join_rate), 18))\n",
    "    ID_gamma_query_b = np.concatenate((ID_gamma_query_b1, ID_gamma_query_b2.view(dtype='|S18').flatten()))\n",
    "    rng.shuffle(ID_gamma_query_b)\n",
    "    df_query_b = pd.DataFrame({'ID': ID_gamma_query_b })\n",
    "    table_query_b = pa.Table.from_pandas(df_query_b)\n",
    "    pq.write_to_dataset(table_query_b, root_path=f'{path}/ds_gamma_query_b')\n",
    "    print(\"ds_gamma_query_b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 29.5 s, sys: 7.56 s, total: 37.1 s\n",
      "Wall time: 1min 49s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# epsilon：400,000 / 100,000 | 2000抽取900 | 特征工程、建模、预测\n",
    "# 采用epsilon数据集，数据集已完成归一化、标准化。40万行样本作为训练集，10万行样本作为测试集。\n",
    "# 原始数据集包含2000个特征，随机抽取900个特征用于计算。\n",
    "# 数据方数量为两个，每个数据方都持有450个特征，只有一个数据方持有标签信息。\n",
    "\n",
    "import random\n",
    "from catboost.datasets import epsilon\n",
    "epsilon_train, epsilon_test = epsilon()\n",
    "epsilon_test.columns = epsilon_test.columns.astype(str)\n",
    "epsilon_train.columns = epsilon_train.columns.astype(str)\n",
    "assert (epsilon_test.columns == epsilon_train.columns).all()\n",
    "columns = list(epsilon_train.columns[1:])\n",
    "random.shuffle(columns)\n",
    "table1_schema = pa.schema([pa.field('0', pa.int8())] + [pa.field(col, pa.float32()) for col in columns[0:450]])\n",
    "table2_schema = pa.schema([pa.field(col, pa.float32()) for col in columns[450:900]])\n",
    "\n",
    "table1_test = pa.Table.from_pandas(epsilon_test, schema=table1_schema)\n",
    "pq.write_to_dataset(table1_test, root_path=f'{path}/ds_epsilon1_test')\n",
    "table2_test = pa.Table.from_pandas(epsilon_test, schema=table2_schema)\n",
    "pq.write_to_dataset(table2_test, root_path=f'{path}/ds_epsilon2_test')\n",
    "\n",
    "table1_train = pa.Table.from_pandas(epsilon_train, schema=table1_schema)\n",
    "pq.write_to_dataset(table1_train, root_path=f'{path}/ds_epsilon1_train')\n",
    "table2_train = pa.Table.from_pandas(epsilon_train, schema=table2_schema)\n",
    "pq.write_to_dataset(table2_train, root_path=f'{path}/ds_epsilon2_train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('pyenv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2cac66f9c3a3e478db6b53d819924530ad4b772b6b649f43a52775cb3a625d7c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
